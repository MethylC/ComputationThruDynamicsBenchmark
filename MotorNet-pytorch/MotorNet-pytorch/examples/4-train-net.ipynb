{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Building a network\n",
    "\n",
    "The purpose of this notebook is to create a network to control an effector, and to train it to perform a task through the optimization process. We will also briefly cover how to save and re-load a network.\n",
    "\n",
    "For how to build an effector and environment from scratch, feel free to look up the `1-build-effector.ipynb` and `3-environments.ipynb` notebooks.\n",
    "\n",
    "Let's start by importing what we need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cell using LOCAL initialization...\n",
      "All packages imported.\n",
      "pytorch version: 2.0.1+cu117\n",
      "numpy version: 1.25.1\n",
      "motornet version: 0.2.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "motornet_in_cwd = os.path.exists(\"MotorNet\") or os.path.exists(\"motornet\")\n",
    "colab_env = 'google.colab' in str(get_ipython()) if hasattr(__builtins__,'__IPYTHON__') else False\n",
    "colab_initialized = True if motornet_in_cwd else False\n",
    "\n",
    "if colab_env and not colab_initialized:\n",
    "  !pip install gymnasium>=2.8\n",
    "  !pip install git+https://github.com/OlivierCodol/MotorNet@pytorch\n",
    "  sys.path.append('MotorNet')\n",
    "  print(\"Running cell using COLAB initialization...\")\n",
    "elif colab_env and colab_initialized:\n",
    "  print(\"Already initialized using COLAB initialization.\")\n",
    "else:\n",
    "  paths = [p for p in sys.path if os.path.exists(p)]\n",
    "  local_initialized = True if [p for p in paths if \"MotorNet\" in os.listdir(p)] else False\n",
    "  if local_initialized:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    print(\"Already initialized using LOCAL initialization.\")\n",
    "  else:\n",
    "    path = [p for p in paths if p.__contains__(\"examples\")]\n",
    "    if len(path) != 1:\n",
    "      raise ValueError(\"Path to MotorNet could not be determined with certainty.\")\n",
    "    else:\n",
    "      path = path[0]\n",
    "    sys.path.append(os.path.dirname(path[:path.rfind('examples')]))\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    print(\"Running cell using LOCAL initialization...\")\n",
    "\n",
    "\n",
    "import motornet as mn\n",
    "\n",
    "\n",
    "print('All packages imported.')\n",
    "print('pytorch version: ' + th.__version__)\n",
    "print('numpy version: ' + np.__version__)\n",
    "print('motornet version: ' + mn.__version__)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "# I. Introduction\n",
    "\n",
    "Since the purpose of this notebook is not to show how to build an effector or an environment, we will use a pre-built effector and environment that comes with the `motornet` toolbox. This is a 4-muscles point mass plant, with `ReluMuscle` actuators, and a simple random-to-random position reaching task.\n",
    "\n",
    "Generally speaking, the objects we create follow the hierarchical structure illustrated below.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "\n",
    "<img src=\"img/hierarchy.png\" alt=\"drawing\" width=\"500\"/>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# II. Building the model\n",
    "\n",
    "## II. 1. `Effector` and `Environment` instances.\n",
    "All the elements below take from previous tutorials, so we are just repeating them here.\n",
    "\n",
    "Note that we specify the maximum duration of an episode to be 1 sec. We set this up explicitly here, but this is actually the default if no value is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "effector = mn.effector.ReluPointMass24()\n",
    "env = mn.environment.RandomTargetReach(effector=effector, max_ep_duration=1.)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. 2. Building the network\n",
    "\n",
    "This step simply reduces to building a `torch.nn.Module` subclass. If you are not sure on how to achieve that, there are many tutorials available online for learning PyTorch basics.\n",
    "\n",
    "A good and simple model for this kind of task is to have one recurrent layer receiving observation inputs, so the input dimension is the size of the observation vector. Here we specify the recurrent layer to be one layer of 32 Gated Recurrent Units (GRUs). The output layer is a simple fully connected linear layer, with a sigmoid non-linearity.\n",
    "\n",
    "Note that we initialize the fully connected layer's bias to be `-5`, to ensure we start the training with low output forces and therefore a more stable situation. Also note that the action that should be input into the environment should be bounded from 0 to 1, which the sigmoid guarantees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Policy(th.nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = 1\n",
    "        \n",
    "        self.gru = th.nn.GRU(input_dim, hidden_dim, 1, batch_first=True)\n",
    "        self.fc = th.nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = th.nn.Sigmoid()\n",
    "\n",
    "        # the default initialization in torch isn't ideal\n",
    "        for name, param in self.named_parameters():\n",
    "            if name == \"gru.weight_ih_l0\":\n",
    "                th.nn.init.xavier_uniform_(param)\n",
    "            elif name == \"gru.weight_hh_l0\":\n",
    "                th.nn.init.orthogonal_(param)\n",
    "            elif name == \"gru.bias_ih_l0\":\n",
    "                th.nn.init.zeros_(param)\n",
    "            elif name == \"gru.bias_hh_l0\":\n",
    "                th.nn.init.zeros_(param)\n",
    "            elif name == \"fc.weight\":\n",
    "                th.nn.init.xavier_uniform_(param)\n",
    "            elif name == \"fc.bias\":\n",
    "                th.nn.init.constant_(param, -5.)\n",
    "            else:\n",
    "                raise ValueError\n",
    "        \n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, h0):\n",
    "        y, h = self.gru(x[:, None, :], h0)\n",
    "        u = self.sigmoid(self.fc(y)).squeeze(dim=1)\n",
    "        return u, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(self.device)\n",
    "        return hidden\n",
    "    \n",
    "device = th.device(\"cpu\")\n",
    "\n",
    "policy = Policy(env.observation_space.shape[0], 32, env.n_muscles, device=device)\n",
    "optimizer = th.optim.Adam(policy.parameters(), lr=10**-3)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then train the `Policy` network like we would any PyTorch model. The output of the `Policy` is then passed as input to the `Environment` object using the `Environment.step()` method. The `obs` and `info` ouputs are then collected and used for the loss computation and the backward pass.\n",
    "\n",
    "Importantly, adding some clipping greatly improves the training process for motor tasks in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m loss \u001b[39m=\u001b[39m l1(xy, tg)  \u001b[39m# L1 loss on position\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39m# backward pass & update weights\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     35\u001b[0m th\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(policy\u001b[39m.\u001b[39mparameters(), max_norm\u001b[39m=\u001b[39m\u001b[39m1.\u001b[39m)  \u001b[39m# important!\u001b[39;00m\n\u001b[1;32m     36\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/motorEnv/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/motorEnv/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 32\n",
    "n_batch = 6000\n",
    "losses = []\n",
    "interval = 50\n",
    "\n",
    "def l1(x, y):\n",
    "  \"\"\"L1 loss\"\"\"\n",
    "  return th.mean(th.sum(th.abs(x - y), dim=-1))\n",
    "\n",
    "for batch in range(n_batch):\n",
    "  # initialize batch\n",
    "  h = policy.init_hidden(batch_size=batch_size)\n",
    "  obs, info = env.reset(batch_size=batch_size)\n",
    "  terminated = False\n",
    "\n",
    "  # initial positions and targets\n",
    "  xy = [info[\"states\"][\"fingertip\"][:, None, :]]\n",
    "  tg = [info[\"goal\"][:, None, :]]\n",
    "\n",
    "  # simulate whole episode\n",
    "  while not terminated:  # will run until `max_ep_duration` is reached\n",
    "    action, h = policy(obs, h)\n",
    "    obs, reward, terminated, truncated, info = env.step(action=action)\n",
    "\n",
    "    xy.append(info[\"states\"][\"fingertip\"][:, None, :])  # trajectories\n",
    "    tg.append(info[\"goal\"][:, None, :])  # targets\n",
    "\n",
    "  # concatenate into a (batch_size, n_timesteps, xy) tensor\n",
    "  xy = th.cat(xy, axis=1)\n",
    "  tg = th.cat(tg, axis=1)\n",
    "  loss = l1(xy, tg)  # L1 loss on position\n",
    "  \n",
    "  # backward pass & update weights\n",
    "  loss.backward()\n",
    "  th.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=1.)  # important!\n",
    "  optimizer.step()\n",
    "  losses.append(loss.item())\n",
    "\n",
    "  if (batch % interval == 0) and (batch != 0):\n",
    "    print(\"Batch {}/{} Done, mean policy loss: {}\".format(batch, n_batch, sum(losses[-interval:])/interval))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_log(log):\n",
    "  fig, axs = plt.subplots(1, 1)\n",
    "  fig.set_tight_layout(True)\n",
    "  fig.set_size_inches((8, 3))\n",
    "\n",
    "  axs.semilogy(log)\n",
    "\n",
    "  axs.set_ylabel(\"Loss\")\n",
    "  axs.set_xlabel(\"Batch #\")\n",
    "  plt.show()\n",
    "\n",
    "plot_training_log(losses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Evaluating performance\n",
    "\n",
    "A forward pass on the model (with `deterministic=True`) will yield the evaluation behaviour.\n",
    "\n",
    "We can plot the resulting states to quickly visualize what the network is doing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotor = mn.plotor.plot_pos_over_time\n",
    "\n",
    "def plot_simulations(xy, target_xy):\n",
    "  target_x = target_xy[:, -1, 0]\n",
    "  target_y = target_xy[:, -1, 1]\n",
    "\n",
    "  plt.figure(figsize=(10,3))\n",
    "\n",
    "  plt.subplot(1,2,1)\n",
    "  plt.ylim([-1.1, 1.1])\n",
    "  plt.xlim([-1.1, 1.1])\n",
    "  plotor(axis=plt.gca(), cart_results=xy)\n",
    "  plt.scatter(target_x, target_y)\n",
    "\n",
    "  plt.subplot(1,2,2)\n",
    "  plt.ylim([-2, 2])\n",
    "  plt.xlim([-2, 2])\n",
    "  plotor(axis=plt.gca(), cart_results=xy - target_xy)\n",
    "  plt.axhline(0, c=\"grey\")\n",
    "  plt.axvline(0, c=\"grey\")\n",
    "  plt.xlabel(\"X distance to target\")\n",
    "  plt.ylabel(\"Y distance to target\")\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "plot_simulations(xy=th.detach(xy), target_xy=th.detach(tg))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Saving the model\n",
    "\n",
    "Next, we can save the network like we would any PyTorch model. We will also save the training history as a `.json` file, and the environment's configuration information for later reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weight_file = os.path.join(\"save\", \"weights\")\n",
    "log_file = os.path.join(\"save\", \"log.json\")\n",
    "cfg_file = os.path.join(\"save\", \"cfg.json\")\n",
    "\n",
    "\n",
    "# save model weights\n",
    "th.save(policy.state_dict(), weight_file)\n",
    "\n",
    "\n",
    "# save training history (log)\n",
    "with open(log_file, 'w') as file:\n",
    "    json.dump(losses, file)\n",
    "\n",
    "\n",
    "# save environment configuration dictionary\n",
    "cfg = env.get_save_config()\n",
    "with open(cfg_file, 'w') as file:\n",
    "    json.dump(cfg, file)\n",
    "\n",
    "\n",
    "print(\"done.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# V. Loading the model\n",
    "\n",
    "## V. 1. The configuration file\n",
    "This file is what the `Environment.get_save_config()` method produces. It contains a dictionary with all the parameter and configuration values used to create the environment. Let's look into its structure quickly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(cfg_file, 'r') as file:\n",
    "    cfg = json.load(file)\n",
    "\n",
    "\n",
    "for k1, v1 in cfg.items():\n",
    "  if isinstance(v1, dict):\n",
    "    print(k1 + \":\")\n",
    "    for k2, v2 in v1.items():\n",
    "      if type(v2) is dict:\n",
    "        print(\"\\t\\t\" + k2 + \":\")\n",
    "        for k3, v3 in v2.items():\n",
    "          print(\"\\t\\t\\t\\t\" + k3 + \": \", v3)\n",
    "      else:\n",
    "        print(\"\\t\\t\" + k2 + \": \", v2)\n",
    "  else:\n",
    "    print(k1 + \": \", v1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## V. 2. Training history\n",
    "\n",
    "We can use the `.json` file containing the training history to retrieve and plot losses over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve and plot training history\n",
    "with open(log_file, 'r') as file:\n",
    "    loaded_training_log = json.load(file)\n",
    "\n",
    "plot_training_log(log=loaded_training_log)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## V. 3. The model itself\n",
    "\n",
    "Usually, reloading the model would involve re-creating the same model, and applying the trained weights to that model again. To re-create the model, one could re-use the same function as was used to create the original model (that's what we will do here), or one could exploit the configuration file we saved and re-loaded above to recreate a model exhaustively. The latter method might be more tedious to implement but has the benefit of being more exhaustive on the long run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env2 = mn.environment.RandomTargetReach(effector=mn.effector.ReluPointMass24(), max_ep_duration=1.)\n",
    "policy2 = Policy(env.observation_space.shape[0], 32, env.n_muscles, device=device)\n",
    "\n",
    "policy2.load_state_dict(th.load(weight_file))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# VI. Evaluating the loaded model\n",
    "\n",
    "We can then do a forward pass the loaded model like we did on the original model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========\n",
    "# Evaluation\n",
    "# ==========\n",
    "\n",
    "# initialize batch\n",
    "h = policy.init_hidden(batch_size=batch_size)\n",
    "obs, info = env.reset(batch_size=batch_size)\n",
    "terminated = False\n",
    "\n",
    "# initial positions and targets\n",
    "xy = [info[\"states\"][\"fingertip\"][:, None, :]]\n",
    "tg = [info[\"goal\"][:, None, :]]\n",
    "\n",
    "# simulate whole episode\n",
    "while not terminated:  # will run until `max_ep_duration` is reached\n",
    "  action, h = policy(obs, h)\n",
    "  obs, reward, terminated, truncated, info = env.step(action=action)\n",
    "\n",
    "  xy.append(info[\"states\"][\"fingertip\"][:, None, :])  # trajectories\n",
    "  tg.append(info[\"goal\"][:, None, :])  # targets\n",
    "\n",
    "# concatenate into a (batch_size, n_timesteps, xy) tensor\n",
    "xy = th.detach(th.cat(xy, axis=1))\n",
    "tg = th.detach(th.cat(tg, axis=1))\n",
    "\n",
    "plot_simulations(xy=xy, target_xy=tg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
